ä½ æ˜¯ä¸€åèµ„æ·± macOS App å¼€å‘è€…ã€‚è¯·ä»é›¶æ„å»ºä¸€ä¸ªåä¸º **TransFlow** çš„ macOS å®æ—¶è¯­éŸ³è½¬å½•åº”ç”¨ã€‚

---

## æŠ€æœ¯æ ˆ

- **macOS 26.0 (Tahoe)**ï¼ŒSwift 6.0ï¼Œä¸¥æ ¼å¹¶å‘å®‰å…¨
- **è½¬å½•**ï¼šåŸç”Ÿ Speech æ¡†æ¶ï¼ˆ`SpeechAnalyzer` + `SpeechTranscriber` + `AssetInventory`ï¼‰â€” ä¸ä½¿ç”¨ä»»ä½•ç¬¬ä¸‰æ–¹ ASR æ¡†æ¶ï¼Œä¸ä½¿ç”¨æ—§ç‰ˆ SFSpeechRecognizer
- **ç¿»è¯‘**ï¼šåŸç”Ÿ Translation æ¡†æ¶
- **éŸ³é¢‘æ•è·**ï¼šAVAudioEngineï¼ˆéº¦å…‹é£ï¼‰+ ScreenCaptureKitï¼ˆApp éŸ³é¢‘ï¼‰
- **UI**ï¼šSwiftUIï¼ŒmacOS Tahoe Liquid Glass é£æ ¼
- **é¡¹ç›®ç®¡ç†**ï¼šXcodeGenï¼ˆ`project.yml`ï¼‰ï¼Œæ— ç¬¬ä¸‰æ–¹ package ä¾èµ–
- é¡¹ç›®ç»“æ„ç”±ä½ æŒ‰æœ€ä½³å®è·µç»„ç»‡ï¼Œæ”¯æŒåç»­åŠŸèƒ½æ‰©å±•

---

## åŠŸèƒ½æ¦‚è¿°

1. **å®æ—¶è½¬å½•**ï¼šéº¦å…‹é£æˆ–æŒ‡å®š App çš„éŸ³é¢‘ â†’ SpeechAnalyzer â†’ å®æ—¶é¢„è§ˆ + è‡ªç„¶æ–­å¥
2. **å¤šè¯­è¨€**ï¼šé€šè¿‡ `SpeechTranscriber.supportedLocales` åŠ¨æ€è·å–æ‰€æœ‰æ”¯æŒçš„è¯­è¨€ï¼Œç”¨æˆ·å¯åˆ‡æ¢
3. **éŸ³é¢‘æºåˆ‡æ¢**ï¼šéº¦å…‹é£ / App éŸ³é¢‘ï¼ˆScreenCaptureKit æ•è·æŒ‡å®šåº”ç”¨éŸ³é¢‘è¾“å‡ºï¼‰
4. **å®æ—¶ç¿»è¯‘**ï¼šä½¿ç”¨ Apple Translation æ¡†æ¶ï¼Œå¯¹å·²å®Œæˆå¥å­å’Œ partial æ–‡æœ¬å®æ—¶ç¿»è¯‘
5. **SRT å¯¼å‡º**ï¼šå°†è½¬å½•å†å²ï¼ˆå«ç¿»è¯‘ï¼‰å¯¼å‡ºä¸º SRT å­—å¹•æ–‡ä»¶

---

## æ ¸å¿ƒï¼šSpeechEngine è½¬å½•å¼•æ“

è¿™æ˜¯æ•´ä¸ªåº”ç”¨æœ€æ ¸å¿ƒçš„éƒ¨åˆ†ã€‚å®Œæ•´å®ç°å¦‚ä¸‹ï¼š

```swift
import Speech
import AVFoundation

/// ä½¿ç”¨ macOS 26.0 SpeechAnalyzer + SpeechTranscriber å®ç°å®æ—¶è½¬å½•ã€‚
/// æ¥å— AudioChunk æµï¼ˆ16kHz mono Float32ï¼‰ï¼Œè¾“å‡º TranscriptionEvent æµã€‚
final class SpeechEngine: Sendable {
    private let locale: Locale

    init(locale: Locale) {
        self.locale = locale
    }

    func processStream(_ audioStream: AsyncStream<AudioChunk>) -> AsyncStream<TranscriptionEvent> {
        let (events, continuation) = AsyncStream<TranscriptionEvent>.makeStream(
            bufferingPolicy: .bufferingNewest(128)
        )
        let locale = self.locale

        Task {
            do {
                // 1. åˆ›å»º SpeechTranscriber
                //    - .volatileResults: è¾“å‡ºå®æ—¶é¢„è§ˆï¼ˆé finalï¼‰æ–‡æœ¬
                //    - .fastResults: åå‘ä½å»¶è¿Ÿ
                guard let supportedLocale = await SpeechTranscriber.supportedLocale(
                    equivalentTo: locale
                ) else {
                    continuation.yield(.error("Language \(locale.identifier) not supported"))
                    continuation.finish()
                    return
                }

                let transcriber = SpeechTranscriber(
                    locale: supportedLocale,
                    transcriptionOptions: [],
                    reportingOptions: [.fastResults, .volatileResults],
                    attributeOptions: []
                )

                // 2. è·å–å…¼å®¹éŸ³é¢‘æ ¼å¼ï¼ˆSpeechAnalyzer å¯èƒ½éœ€è¦é 16kHz æ ¼å¼ï¼‰
                let analyzerFormat = await SpeechAnalyzer.bestAvailableAudioFormat(
                    compatibleWith: [transcriber]
                )

                // 3. ç¡®ä¿è¯­éŸ³æ¨¡å‹èµ„äº§å·²å®‰è£…ï¼ˆç³»ç»Ÿç®¡ç†ï¼Œé¦–æ¬¡ä½¿ç”¨è‡ªåŠ¨ä¸‹è½½ï¼‰
                if let installRequest = try await AssetInventory.assetInstallationRequest(
                    supporting: [transcriber]
                ) {
                    try await installRequest.downloadAndInstall()
                }

                // 4. åˆ›å»º SpeechAnalyzer å¹¶é¢„çƒ­ï¼ˆå‡å°‘é¦–æ¬¡ç»“æœå»¶è¿Ÿï¼‰
                let analyzer = SpeechAnalyzer(modules: [transcriber])
                try await analyzer.prepareToAnalyze(in: analyzerFormat)

                // 5. åˆ›å»ºè¾“å…¥æµ
                let (inputSequence, inputBuilder) = AsyncStream<AnalyzerInput>.makeStream()

                // 6. éŸ³é¢‘æ ¼å¼è½¬æ¢å™¨ï¼ˆ16kHz â†’ analyzer æ ¼å¼ï¼‰
                let sourceFormat = AVAudioFormat(
                    standardFormatWithSampleRate: 16_000, channels: 1
                )!
                let converter: AVAudioConverter? = if let analyzerFormat {
                    AVAudioConverter(from: sourceFormat, to: analyzerFormat)
                } else { nil }

                // é¢„åˆ†é…å¯å¤ç”¨è¾“å‡º buffer
                let reusableBuffer: AVAudioPCMBuffer?
                if let converter, let analyzerFormat {
                    let ratio = analyzerFormat.sampleRate / sourceFormat.sampleRate
                    let capacity = AVAudioFrameCount(16_000 * 0.25 * ratio) + 64
                    reusableBuffer = AVAudioPCMBuffer(pcmFormat: analyzerFormat, frameCapacity: capacity)
                } else { reusableBuffer = nil }

                // 7. å…ˆå¯åŠ¨ç»“æœæ¶ˆè´¹ï¼ˆé¿å…ä¸¢å¤±æ—©æœŸç»“æœï¼‰
                //    result.isFinal == true  â†’ å¥å­ç¡®å®šï¼ˆsentenceCompleteï¼‰
                //    result.isFinal == false â†’ å®æ—¶é¢„è§ˆï¼ˆpartialï¼‰
                //    result.text æ˜¯ AttributedStringï¼Œéœ€ String(result.text.characters) è½¬çº¯æ–‡æœ¬
                let resultTask = Task(priority: .userInitiated) {
                    do {
                        for try await result in transcriber.results {
                            let text = String(result.text.characters)
                            if result.isFinal {
                                let trimmed = text.trimmingCharacters(in: .whitespacesAndNewlines)
                                if !trimmed.isEmpty {
                                    continuation.yield(.sentenceComplete(
                                        TranscriptionSentence(timestamp: Date(), text: trimmed)
                                    ))
                                    continuation.yield(.partial(""))
                                }
                            } else {
                                continuation.yield(.partial(text))
                            }
                        }
                    } catch {
                        continuation.yield(.error("Speech error: \(error.localizedDescription)"))
                    }
                }

                // 8. å¯åŠ¨è‡ªæ²»åˆ†æï¼ˆç«‹å³è¿”å›ï¼‰
                try await analyzer.start(inputSequence: inputSequence)

                // 9. å–‚å…¥éŸ³é¢‘ï¼šç´¯ç§¯ ~200ms å†æ‰¹é‡é€å…¥ï¼Œå‡å°‘æ ¼å¼è½¬æ¢å¼€é”€
                var accumulator: [Float] = []
                let batchThreshold = Int(16_000 * 0.2)

                for await chunk in audioStream {
                    accumulator.append(contentsOf: chunk.samples)
                    guard accumulator.count >= batchThreshold else { continue }

                    if let input = Self.convertToAnalyzerInput(
                        samples: accumulator, converter: converter, reusableBuffer: reusableBuffer
                    ) {
                        inputBuilder.yield(input)
                    }
                    accumulator.removeAll(keepingCapacity: true)
                }

                // flush å‰©ä½™
                if !accumulator.isEmpty, let input = Self.convertToAnalyzerInput(
                    samples: accumulator, converter: converter, reusableBuffer: reusableBuffer
                ) {
                    inputBuilder.yield(input)
                }

                // 10. å®Œæˆ
                inputBuilder.finish()
                try await analyzer.finalizeAndFinishThroughEndOfInput()
                resultTask.cancel()

            } catch {
                continuation.yield(.error("Engine error: \(error.localizedDescription)"))
            }
            continuation.finish()
        }

        return events
    }

    // MARK: - Helpers

    /// å°† Float32 samples è½¬æ¢ä¸º AnalyzerInputï¼ˆå«æ ¼å¼è½¬æ¢ï¼‰
    private static func convertToAnalyzerInput(
        samples: [Float],
        converter: AVAudioConverter?,
        reusableBuffer: AVAudioPCMBuffer?
    ) -> AnalyzerInput? {
        guard let pcmBuffer = createPCMBuffer(from: samples) else { return nil }

        if let converter, let reusableBuffer {
            reusableBuffer.frameLength = 0
            var error: NSError?
            nonisolated(unsafe) var consumed = false
            converter.convert(to: reusableBuffer, error: &error) { _, outStatus in
                if consumed { outStatus.pointee = .noDataNow; return nil }
                consumed = true; outStatus.pointee = .haveData; return pcmBuffer
            }
            guard error == nil, reusableBuffer.frameLength > 0 else { return nil }
            return AnalyzerInput(buffer: reusableBuffer)
        } else {
            return AnalyzerInput(buffer: pcmBuffer)
        }
    }

    /// ä» Float32 samples åˆ›å»º 16kHz mono PCM buffer
    private static func createPCMBuffer(from samples: [Float]) -> AVAudioPCMBuffer? {
        guard let format = AVAudioFormat(standardFormatWithSampleRate: 16_000, channels: 1),
              let buffer = AVAudioPCMBuffer(pcmFormat: format, frameCapacity: AVAudioFrameCount(samples.count))
        else { return nil }
        buffer.frameLength = AVAudioFrameCount(samples.count)
        guard let channelData = buffer.floatChannelData else { return nil }
        samples.withUnsafeBufferPointer { ptr in
            channelData[0].initialize(from: ptr.baseAddress!, count: samples.count)
        }
        return buffer
    }
}
```

### å…³é”® API é€ŸæŸ¥

| API | è¯´æ˜ |
|-----|------|
| `SpeechTranscriber(locale:transcriptionOptions:reportingOptions:attributeOptions:)` | åˆ›å»ºè½¬å½•å™¨ |
| `.fastResults` | ä½å»¶è¿Ÿä¼˜å…ˆ |
| `.volatileResults` | è¾“å‡ºå®æ—¶é¢„è§ˆï¼ˆé finalï¼‰ç»“æœ |
| `result.isFinal` | true=å¥å­ç¡®å®šï¼Œfalse=volatile é¢„è§ˆ |
| `result.text` | AttributedStringï¼Œç”¨ `String(result.text.characters)` è½¬çº¯æ–‡æœ¬ |
| `SpeechTranscriber.supportedLocale(equivalentTo:)` | æŸ¥æ‰¾åŒ¹é…çš„æ”¯æŒè¯­è¨€ |
| `SpeechTranscriber.supportedLocales` | æ‰€æœ‰æ”¯æŒçš„è¯­è¨€ï¼ˆåŠ¨æ€è·å–ï¼‰ |
| `AssetInventory.assetInstallationRequest(supporting:)` | nil=å·²å®‰è£…ï¼Œå¦åˆ™éœ€ä¸‹è½½ |
| `SpeechAnalyzer.bestAvailableAudioFormat(compatibleWith:)` | è·å–å…¼å®¹éŸ³é¢‘æ ¼å¼ |
| `analyzer.prepareToAnalyze(in:)` | é¢„çƒ­æ¨¡å‹ |
| `analyzer.start(inputSequence:)` | å¯åŠ¨è‡ªæ²»åˆ†æ |
| `analyzer.finalizeAndFinishThroughEndOfInput()` | ç»“æŸåˆ†æ |
| `AnalyzerInput(buffer:)` | åŒ…è£… AVAudioPCMBuffer |

---

## éŸ³é¢‘æ•è·

### éº¦å…‹é£ (AVAudioEngine)

- ä½¿ç”¨ `AVAudioEngine` çš„ `inputNode.installTap` æ•è·éº¦å…‹é£
- é€šè¿‡ `AVAudioConverter` è½¬æ¢ä¸º **16kHz mono Float32**
- æ¯ ~100ms è¾“å‡ºä¸€ä¸ª `AudioChunk`ï¼ˆå« samples + å½’ä¸€åŒ–éŸ³é‡ level + timestampï¼‰
- éŸ³é‡è®¡ç®—ï¼šRMS â†’ dB â†’ å½’ä¸€åŒ–åˆ° 0-1
- æƒé™è¯·æ±‚ï¼š`AVAudioApplication.requestRecordPermission()`
- è¿”å› `(stream: AsyncStream<AudioChunk>, stop: @Sendable () -> Void)`

### App éŸ³é¢‘ (ScreenCaptureKit)

- é€šè¿‡ `SCShareableContent` æšä¸¾å¯ç”¨çš„ GUI åº”ç”¨ï¼ˆæ’é™¤è‡ªèº«ã€éšè—åº”ç”¨ï¼‰
- ä¸ `NSWorkspace.shared.runningApplications` äº¤å‰éªŒè¯ï¼Œè·å–åº”ç”¨å›¾æ ‡
- ä½¿ç”¨ `SCStream` + `SCContentFilter` æ•è·ç›®æ ‡åº”ç”¨éŸ³é¢‘
- é…ç½®ï¼š`capturesAudio = true`ï¼Œ`excludesCurrentProcessAudio = true`ï¼Œæœ€å°åŒ–è§†é¢‘å¼€é”€ï¼ˆ2x2 åƒç´ ï¼Œ1fpsï¼‰
- é€šè¿‡ `SCStreamOutput` delegate æ¥æ”¶ `CMSampleBuffer`ï¼Œè½¬æ¢ä¸º 16kHz mono Float32 `AudioChunk`
- éœ€è¦ Screen & System Audio Recording æƒé™ï¼ˆç³»ç»Ÿè‡ªåŠ¨æç¤ºï¼‰

---

## ç¿»è¯‘ (Apple Translation)

- é€šè¿‡ SwiftUI `.translationTask(configuration, action:)` è·å– `TranslationSession`
- ä¸èƒ½ç›´æ¥åˆ›å»º sessionï¼Œå¿…é¡»é€šè¿‡æ­¤ä¿®é¥°ç¬¦
- å½“ç¿»è¯‘å¼€å…³å¼€å¯æˆ–ç›®æ ‡è¯­è¨€å˜åŒ–æ—¶ï¼Œ`Configuration.invalidate()` + é‡æ–°åˆ›å»ºè§¦å‘æ–° session
- å¯¹å·²å®Œæˆå¥å­ç›´æ¥ç¿»è¯‘
- å¯¹ partial æ–‡æœ¬ä½¿ç”¨ ~300ms debounce ç¿»è¯‘ï¼ˆé¿å…é¢‘ç¹è°ƒç”¨ï¼‰
- æ”¯æŒæ‰¹é‡ç¿»è¯‘ï¼ˆ`session.translate(batch:)`ï¼‰

---

## ViewModel æ ¸å¿ƒé€»è¾‘

`@Observable @MainActor` çš„ ViewModel åè°ƒæ‰€æœ‰æœåŠ¡ï¼š

### æ ¸å¿ƒæµç¨‹

1. **åˆå§‹åŒ–**ï¼šè¯·æ±‚éº¦å…‹é£æƒé™
2. **è¯­è¨€åˆ‡æ¢**ï¼šåœæ­¢ç›‘å¬ â†’ é‡å»º `SpeechEngine(locale:)` â†’ å¯é€‰æ£€æŸ¥èµ„äº§çŠ¶æ€
3. **å¼€å§‹ç›‘å¬**ï¼š
   - æ ¹æ®éŸ³é¢‘æºå¯åŠ¨ AudioCaptureService æˆ– AppAudioCaptureService
   - åˆ›å»º SpeechEngine â†’ `processStream(audioStream)`
   - fork éŸ³é¢‘æµï¼šä¸€è·¯ç»™å¼•æ“è½¬å½•ï¼Œä¸€è·¯æ›´æ–° UI éŸ³é‡æ˜¾ç¤º
   - æ¶ˆè´¹ `TranscriptionEvent` æµæ›´æ–° UIï¼ˆpartial â†’ é¢„è§ˆåŒºï¼ŒsentenceComplete â†’ å†å²åŒºï¼‰
4. **åœæ­¢ç›‘å¬**ï¼šåœæ­¢éŸ³é¢‘æ•è·ï¼Œflush å‰©ä½™ partial text ä¸ºæœ€ç»ˆå¥å­
5. **ç¿»è¯‘è”åŠ¨**ï¼šsentenceComplete æ—¶è‡ªåŠ¨ç¿»è¯‘ï¼Œpartial text debounce ç¿»è¯‘

### å…³é”®çŠ¶æ€

```
sentences: [TranscriptionSentence]  // å·²å®Œæˆå¥å­å†å²
currentPartialText: String          // å®æ—¶é¢„è§ˆ
listeningState: ListeningState      // idle/starting/active/stopping
audioLevel: Float                   // å½“å‰éŸ³é‡ 0-1
audioLevelHistory: [Float]          // æ³¢å½¢å†å²
audioSource: AudioSourceType        // éº¦å…‹é£ or App éŸ³é¢‘
selectedLanguage                    // å½“å‰è½¬å½•è¯­è¨€
isTranslationEnabled: Bool          // ç¿»è¯‘å¼€å…³
translationTargetLanguage           // ç¿»è¯‘ç›®æ ‡è¯­è¨€
currentPartialTranslation: String   // partial æ–‡æœ¬çš„ç¿»è¯‘
errorMessage: String?               // é”™è¯¯æç¤º
```

---

## ç•Œé¢å¸ƒå±€

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                         â”‚
â”‚           è½¬å½•åŒºåŸŸï¼ˆå æ»¡ä¸Šæ–¹ï¼‰              â”‚
â”‚                                         â”‚
â”‚   [14:32:05] This is a sentence.        â”‚
â”‚              è¿™æ˜¯ä¸€ä¸ªå¥å­ã€‚               â”‚
â”‚   [14:32:08] Another complete sentence. â”‚
â”‚              å¦ä¸€ä¸ªå®Œæ•´å¥å­ã€‚              â”‚
â”‚                                         â”‚
â”‚   â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„â”„   â”‚
â”‚   This is being spoken right now...     â”‚  â† volatile é¢„è§ˆï¼ˆç°è‰²/æ–œä½“ï¼‰
â”‚   æ­£åœ¨å®æ—¶ç¿»è¯‘...                        â”‚  â† ç¿»è¯‘é¢„è§ˆ
â”‚                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  âº Start â”‚ ğŸ¤ Mic â–¾ â”‚ EN â–¾ â”‚ ğŸŒ â†’ ZH â–¾ â”‚  â† åº•éƒ¨æ§åˆ¶æ ï¼ˆåˆå¹¶ï¼‰
â”‚  â–â–‚â–ƒâ–…â–ƒâ–‚â–  â”‚  2ms     â”‚ å¯¼å‡º  â”‚           â”‚  â† æ³¢å½¢ + å»¶è¿Ÿ + å¯¼å‡º
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### å¸ƒå±€è¯´æ˜

- **ä¸Šæ–¹è½¬å½•åŒºåŸŸ**å æ®çª—å£ç»å¤§éƒ¨åˆ†ç©ºé—´ï¼ˆScrollView + è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨ï¼‰
  - å·²å®Œæˆå¥å­ï¼šæ—¶é—´æˆ³ + åŸæ–‡ + ç¿»è¯‘æ–‡æœ¬ï¼ˆå¦‚å¯ç”¨ï¼‰
  - åº•éƒ¨ volatile é¢„è§ˆåŒºï¼šç°è‰²/æ–œä½“ï¼Œæ˜¾ç¤ºæ­£åœ¨è¯´çš„å†…å®¹
- **åº•éƒ¨æ§åˆ¶æ **åˆå¹¶ä¸ºä¸€è¡Œï¼ŒåŒ…å«ï¼š
  - å¼€å§‹/åœæ­¢ç›‘å¬æŒ‰é’®
  - éŸ³é¢‘æºåˆ‡æ¢ï¼ˆéº¦å…‹é£ / App éŸ³é¢‘é€‰æ‹©å™¨ï¼‰
  - è½¬å½•è¯­è¨€é€‰æ‹©å™¨
  - ç¿»è¯‘å¼€å…³ + ç›®æ ‡è¯­è¨€é€‰æ‹©å™¨
  - å°å‹éŸ³é‡æ³¢å½¢å¯è§†åŒ–
  - å»¶è¿Ÿæ˜¾ç¤º
  - SRT å¯¼å‡ºæŒ‰é’®
  - é”™è¯¯æç¤º

### èœå•å¿«æ·é”®

- `Cmd+K`ï¼šæ¸…ç©ºå†å²
- `Cmd+Shift+E`ï¼šå¯¼å‡º SRT

### è®¾è®¡åŸåˆ™

- macOS Tahoe Liquid Glass é£æ ¼
- æ·±è‰²/æµ…è‰²æ¨¡å¼è‡ªé€‚åº”
- UI ç»†èŠ‚è‡ªç”±å‘æŒ¥ï¼Œä¿æŒç®€æ´ç¾è§‚
- é‡ç‚¹æ˜¯åŠŸèƒ½å®Œæ•´å¯ç”¨

---

## æ³¨æ„äº‹é¡¹

1. SpeechAnalyzer æ˜¯ **actor** ç±»å‹ï¼ŒSpeechTranscriber æ˜¯ Sendable class â€” æ³¨æ„ Swift 6 å¹¶å‘å®‰å…¨
2. éŸ³é¢‘æ•è·è¾“å‡º 16kHz mono Float32ï¼ŒSpeechAnalyzer å¯èƒ½éœ€è¦ä¸åŒæ ¼å¼ â€” å¿…é¡»ç”¨ `bestAvailableAudioFormat` è·å–å¹¶è½¬æ¢
3. è¯­éŸ³æ¨¡å‹èµ„äº§ç”±ç³»ç»Ÿç®¡ç†ï¼Œä¸‹è½½å app é—´å…±äº«ï¼Œé•¿æœŸä¸ç”¨å¯èƒ½è¢«ç³»ç»Ÿæ¸…ç†
4. Translation session åªèƒ½é€šè¿‡ SwiftUI `.translationTask` ä¿®é¥°ç¬¦è·å–
5. ScreenCaptureKit éœ€è¦ Screen & System Audio Recording æƒé™
6. æ‰€æœ‰ Speech æ¡†æ¶æ–° APIï¼ˆSpeechAnalyzer, SpeechTranscriber, AssetInventory, AnalyzerInput ç­‰ï¼‰å‡ä¸º macOS 26.0+ ä¸“å±
